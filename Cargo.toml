[package]
name = "charabia"
version = "0.6.0"
license = "MIT"
authors = ["Many <many@meilisearch.com>"]
edition = "2021"
description = "A simple library to detect the language, tokenize the text and normalize the tokens"
documentation = "https://docs.rs/charabia"
repository = "https://github.com/meilisearch/charabia"
keywords = ["segmenter", "tokenizer", "normalize", "language"]
categories = ["text-processing"]
exclude = ["/dictionaries/txt/"]

[dependencies]
cow-utils = "0.1"
deunicode = "1.1.1"
fst = "0.4"
jieba-rs = { version = "0.6", optional = true }
once_cell = "1.5.2"
slice-group-by = "0.3.0"
unicode-segmentation = "1.6.0"
whatlang = "0.16.1"
lindera = { version = "=0.17.0", default-features = false, optional = true }
pinyin = { version = "0.9", default-features = false, features = ["with_tone"], optional = true }
wana_kana = { version = "2.1.0", optional = true }
regex = "1"

[features]
default = ["chinese", "hebrew", "japanese", "thai", "korean"]

# allow chinese specialized tokenization
chinese = ["dep:pinyin", "dep:jieba-rs"]

# allow hebrew specialized tokenization
hebrew = []

# allow japanese specialized tokenization
japanese = ["lindera/ipadic"]
japanese-transliteration = ["dep:wana_kana"]

# allow korean specialized tokenization
korean = ["lindera/ko-dic"]

# allow thai specialized tokenization
thai = []

[dev-dependencies]
criterion = "0.3"
jemallocator = "0.3.0"

[[bench]]
name = "bench"
harness = false
